{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_full(x):\n",
    "    pd.set_option('display.max_rows', len(x))\n",
    "    print(x)\n",
    "    pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confidence Interval\n",
    "\n",
    "def get_95_ci(array_1, array_2):\n",
    "    sample_1_n = array_1.shape[0]\n",
    "    sample_2_n = array_2.shape[0]\n",
    "    sample_1_mean = array_1.mean()\n",
    "    sample_2_mean = array_2.mean()\n",
    "    sample_1_var = array_1.var()\n",
    "    sample_2_var = array_2.var()\n",
    "    mean_difference = sample_2_mean - sample_1_mean\n",
    "    std_err_difference = math.sqrt((sample_1_var/sample_1_n)+(sample_2_var/sample_2_n))\n",
    "    margin_of_error = 1.96 * std_err_difference\n",
    "    ci_lower = mean_difference - margin_of_error\n",
    "    ci_upper = mean_difference + margin_of_error\n",
    "    return(\"The difference in means at the 95% confidence interval (two-tail) is between \"+str(ci_lower)+\" and \"+str(ci_upper)+\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert Y / N values into percentage\n",
    "def YNrate(column):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-3-52e322a0a562>, line 21)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-52e322a0a562>\"\u001b[0;36m, line \u001b[0;32m21\u001b[0m\n\u001b[0;31m    for i in range(0, 2):\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# unfinished function to print an assesment of missing values for each column of a data frame\n",
    "\n",
    "def data_missing_values_assesment(frame):\n",
    "    for column in frame.columns:\n",
    "        print('The data type of column {} is {}'.formdata_assesmentat(column, frame[column].dtype))\n",
    "        print('There are {} null values'.format(frame[column].isnull().count()))\n",
    "        problems = []\n",
    "        if frame[column].dtype == 'object':\n",
    "            for value in frame[column]:\n",
    "                string = ''\n",
    "                try:\n",
    "                    string.append(value)\n",
    "                except:\n",
    "                    problems.append(value)\n",
    "        if frame[column].dtype == 'float' or 'float64' or 'int':\n",
    "        \n",
    "        \n",
    "# code above is based on this. chopped up the frame manually because of the different data types\n",
    "# function would handle this as is the case with the object type condition above\n",
    "        \n",
    "for i in range(0, 2):\n",
    "    print('The data type for column: {} is : {}'.format(log.columns[i], log[log.columns[i]].dtype))\n",
    "    print('There are {} unique values'.format(log[log.columns[i]].nunique()))\n",
    "    print('Problem values in are:')\n",
    "    string = ''\n",
    "    for value in log[log.columns[i]]:\n",
    "        try:\n",
    "            string + value\n",
    "        except:\n",
    "            print(value)\n",
    "    print('\\n')\n",
    "\n",
    "for i in range(2, len(log.columns)):\n",
    "    print('The data type for column: {} is : {}'.format(log.columns[i], log[log.columns[i]].dtype))\n",
    "    print('There are {} unique values'.format(log[log.columns[i]].nunique()))\n",
    "    print('Problem values in are:')\n",
    "    for value in log[log.columns[i]]:\n",
    "        try:\n",
    "            float(value)\n",
    "        except:\n",
    "            print(value)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraps for a function to identify outliers\n",
    "\n",
    "for column in kept.select_dtypes(include=['float64']):\n",
    "    print(column)\n",
    "    for threshold in [95, 90, 85, 80, 75]:\n",
    "        alpha = 100 - threshold\n",
    "        upper_limit = 100 - alpha/2\n",
    "        lower_limit = 0 + alpha/2\n",
    "        upper_value, lower_value = np.percentile(kept[column], [upper_limit ,lower_limit])\n",
    "        print('Beyond the inner {}th percentile there are {} outliers'.format(threshold, \n",
    "            len((np.where((kept[column] > upper_value) \n",
    "                      | (kept[column] < lower_value))[0]))\n",
    "        ))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_outliers(lower_inner_upper, df):\n",
    "    \n",
    "    if lower_inner_upper == 'lower':\n",
    "        \n",
    "        print('Continuious Variables', '\\n')\n",
    "\n",
    "        for column in df.select_dtypes(include='number'):\n",
    "            print(column)\n",
    "            for threshold in [95, 90, 85, 80, 75]:\n",
    "                placeholder, lower_bound = np.percentile(df[column], [100, threshold])[0]\n",
    "                \n",
    "                print('Beyond the upper {}th percentile there are {} outliers'.format(threshold, \n",
    "                    len((np.where((df[column] < lower_bound))[0]))\n",
    "                ))\n",
    "                \n",
    "            if (df[column].nunique() < 5):\n",
    "                print('\\n')\n",
    "                print('Possible Categorical Variable')\n",
    "                print('There are {} unique values'.format(df[column].nunique()))\n",
    "                print(df[column].value_counts())\n",
    "            print('\\n')\n",
    "    \n",
    "    elif lower_inner_upper == 'inner':\n",
    "        \n",
    "        print('Continuious Variables', '\\n')\n",
    "    \n",
    "        for column in df_features.select_dtypes(include='number'):\n",
    "        print(column)\n",
    "        for threshold in [95, 90, 85, 80, 75]:\n",
    "            alpha = 100 - threshold\n",
    "            upper_bound, lower_bound = np.percentile(df_features[column]\n",
    "                                                     , [(100 - alpha/2), (0 + alpha/2)]) \n",
    "            \n",
    "            print(\"Beyond the inner {}th percentile there are {} outliers.\".format(threshold,\n",
    "                    len((np.where((df_features[column] > upper_bound) \n",
    "                                  | (df_features[column] < lower_bound))[0]))\n",
    "                ))\n",
    "            \n",
    "        if (df[column].nunique() < 5):\n",
    "                print('\\n')\n",
    "                print('Possible Categorical Variable')\n",
    "                print('There are {} unique values'.format(df[column].nunique()))\n",
    "                print(df[column].value_counts())\n",
    "            print('\\n')\n",
    "            \n",
    "    elif lower_inner_upper == 'upper':\n",
    "    \n",
    "        print('Continuious Variables', '\\n')\n",
    "\n",
    "        for column in df.select_dtypes(include='number'):\n",
    "            print(column)\n",
    "            for threshold in [95, 90, 85, 80, 75]:\n",
    "                upper_bound = np.percentile(df[column], [threshold])[0]\n",
    "                \n",
    "                print('Beyond the lower {}th percentile there are {} outliers'.format(threshold, \n",
    "                    len((np.where((df[column] > upper_bound))[0]))\n",
    "                ))\n",
    "                \n",
    "            if (df[column].nunique() < 5):\n",
    "                print('\\n')\n",
    "                print('Possible Categorical Variable')\n",
    "                print('There are {} unique values'.format(df[column].nunique()))\n",
    "                print(df[column].value_counts())\n",
    "            print('\\n')\n",
    "        \n",
    "        \n",
    "    \n",
    "    print('Categorical Variables', '\\n')\n",
    "    \n",
    "    for column in df.select_dtypes(include='object'):\n",
    "        print(column)\n",
    "        print('There are {} unique values'.format(df[column].nunique()))\n",
    "        if df[column].nunique() < 20:\n",
    "            print(df[column].value_counts())\n",
    "            print('\\n')\n",
    "    \n",
    "    ## inner quartile range\n",
    "for column in treat_columns:\n",
    "    q75, q25 = np.percentile(kept[column], [75 ,25])\n",
    "    iqr = q75 - q25\n",
    "    print(column)\n",
    "    for threshold in np.arange(1,5,0.5):\n",
    "        min_val = q25 - (iqr*threshold)\n",
    "        max_val = q75 + (iqr*threshold)\n",
    "        print(\"The score threshold is: {}\".format(threshold))\n",
    "        print(\"Number of outliers is: {}\".format(\n",
    "            len((np.where((kept[column] > max_val) \n",
    "                          | (kept[column] < min_val))[0]))\n",
    "        ))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two functions above and below this cell should be combined. One line of code to look at all the variables, sort them by categorical or continuous, print dubious values and outliers for each. \n",
    "\n",
    "Ideally this function would then present the user with the unsorted variables. \n",
    "\n",
    "Need to find a way to deal with timeseries data and things like unique id and labels that we want to preserve and be able to index back to the variables after transformation but don't want or need when doing EDA, transformation and modeling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cats_conts_unsorted(df):\n",
    "     \n",
    "        # Some paramater tuning is no doubt called for at some point. For now these are rough\n",
    "        # values for sorting and we will clean up the rest manually. Better than doing the whole\n",
    "        # thing manually!\n",
    "        \n",
    "    cats = []\n",
    "    conts = []\n",
    "    unsorted = []\n",
    "    \n",
    "    \n",
    "    for column in df.columns:\n",
    "        if df[column].nunique() < 10:\n",
    "            cats.append(column)\n",
    "        \n",
    "        elif df[column].nunique() < 50 or df[column].nunique() <  len(df)/10:\n",
    "            unsorted.append(column)\n",
    "        \n",
    "        else:\n",
    "            conts.append(column)\n",
    "            \n",
    "            # there is some room here to integrate a process for mauallyl assiging categorical\n",
    "            # or continuous to the unsorted columns.\n",
    "            \n",
    "            \n",
    "    print('{} cats'.format(len(cats))\n",
    "         , '\\n', '{} conts'.format(len(conts))\n",
    "         , '\\n', '{} unsorted'.format(len(unsorted))\n",
    "         )\n",
    "    \n",
    "    return cats, conts, unsorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multivariate_target_analysis(df, target, cat_or_cont):\n",
    "    # excluding the target determine which columns are continuous, categorical, and date/time variables\n",
    "    \n",
    "    # treat some int/float dtypes as cat when they have below n/2 unique values\n",
    "    \n",
    "    \n",
    "    if cat_or_cont == 'cat':\n",
    "    \n",
    "    elif cat_or_cont == 'cont':\n",
    "\n",
    "\n",
    "# cat / cat\n",
    "\n",
    "# cat / cont\n",
    "\n",
    "\n",
    "# cont / cont\n",
    "\n",
    "# cont / cat\n",
    "target = 'boxcox average'\n",
    "\n",
    "variables = winsorized.select_dtypes(include='object')\n",
    "\n",
    "for variable in variables:\n",
    "        print(variable, '\\n'\n",
    "             , '-----------------------------------------------------')\n",
    "        categories = df[variable].unique()\n",
    "        for i in range(len(categories)):\n",
    "            for j in range(i+1, len(categories)):\n",
    "                print('t-test results between {}, and {}:'.format(categories[i], categories[j]))\n",
    "                print(stats.ttest_ind(winsorized[winsorized[variable] == categories[i]][target]\n",
    "                             , winsorized[winsorized[variable] == categories[j]][target]))\n",
    "                print('\\n')\n",
    "        print('\\n')           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "multivariate_visualization() got an unexpected keyword argument 'cat_or_cont'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-0490d5562d35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmultivariate_visualization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaleprice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat_or_cont\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: multivariate_visualization() got an unexpected keyword argument 'cat_or_cont'"
     ]
    }
   ],
   "source": [
    "multivariate_visualization(df, saleprice, cat_or_cont='cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multivariate_visualization(df, target, cats, conts):\n",
    "\n",
    "    if target in conts:\n",
    "        \n",
    "        # cont / cont\n",
    "        \n",
    "        map_df = df.loc[:, conts]\n",
    "        map_df.set_index(target)\n",
    "        \n",
    "        plt.figure(figsize=(len(conts)*2, len(conts)*2))\n",
    "        corr_map = map_df.corr()\n",
    "        sns.heatmap(corr_map, square=True, annot=True, linewidths=.5)\n",
    "        \n",
    "        \n",
    "        # cont / cat\n",
    "        plt.figure(figsize=(15, len(cats)*4))\n",
    "\n",
    "        for i, feature in enumerate(cats):\n",
    "            plt.subplot(len(cats), 1, i+1)\n",
    "            sns.barplot(df[feature], df[target])\n",
    "            plt.title(feature)\n",
    "            plt.xlabel('')\n",
    "            plt.xticks(rotation=90)\n",
    "        plt.tight_layout(pad=0.3)\n",
    "        plt.show()\n",
    "\n",
    "    # elif target in cats:\n",
    "    \n",
    "    # need to write second half of the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_nulls(df):\n",
    "    return df.isnull().sum()*100/df.isnull().isnull().count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
